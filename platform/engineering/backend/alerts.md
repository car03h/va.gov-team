# Alerts

This document describes alerts generated by Vets.gov monitoring systems.  See the [On Call](/Practice%20Areas/Engineering/OnCall) documentation for more information on how the team is notified of and responds to these alerts.

Vets.gov contains two alert generation mechanisms - the Prometheus Alertmanager
and CloudWatch Alarms. The Prometheus Alertmanager is responsible for alerts
based on metrics Prometheus has collected from the available metrics exporters.
Where metrics are not currently collected, or for alerts on AWS specific
functionality, CloudWatch alarms are configured. All alerts are routed to the
relevant team via PagerDuty and or Slack.

## Alert Categorization

The alerts raised by VSP are categorized across two dimensions: Criticality and Urgency. 

Alerts categorized as "Critical" mean that something is in a critical or broken state. A "Warning" means that it a given metric is an abnormal-but-not-critical state or could become critical in the near future if no action is taken. Those decisions are made by alert rules in Prometheus using tags (or CloudWatch with static configuration) and used to determine the alert name which links to this document.

The urgency of an alert is determined at the service level inside PagerDuty. Events are either high or low urgency, and this is determined based upon the Alert Manager configuration. High urgency incidents should scream loudly at an engineer and allow the system to wake them up or interrupt work. Low urgency alerts can be sent to email/Slack. Note: the wording is currently confusing in PagerDuty as the two (urgency/criticality) are conflated, fixit ticket is [here](https://github.com/department-of-veterans-affairs/va.gov-team/issues/3092).

## Prometheus Alerts

A Prometheus instance is available in each application environment and within
the Utilty VPC. Instances in the application environment are hosted on
us-gov-west-1a, and the Utility VPC's instance is hosted on us-gov-west-1b. The
Prometheus Alertmanager is a separate process that runs alongside each of these
instances. The Prometheus server evaluates Alert Rules, which are sent to the
Alertmanager for routing.

With Prometheus HA (#955) this configuration may change, as the Alertmanager
process will run on its own system and route fired alerts from multiple
Prometheus servers in each availability zone.

The Alertmanager groups alerts by their `alertname` and `job`. It will pause 30
seconds after an initial alert is fired to collect additional alerts for
grouping purposes. See the [alert manager grouping and routing
configuration](https://github.com/department-of-veterans-affairs/devops/blob/master/ansible/deployment/config/prometheus/alertmanager.yml.j2)
and [Prometheus Alertmanager
documentation](https://prometheus.io/docs/alerting/overview/).

Alerts are either Critical, or Warning. The production environment will route
Critical alerts to DSVA's PagerDuty account, where scheduling features and
escalation policies are defined. Other alerts are routed to the [DevOps
Alerts](https://dsva.slack.com/messages/devops-alerts/) channel on Slack.
Since critical alerts may be routed to PagerDuty, they are limited only to
issues that may either severely impact or will soon impact site security,
reachability or performance such that the internal SLA is broken.

### API Gateway Alerts
TODO: Flesh out API gateway on-call playbook
#### ApiGatewayHighCPUCritical  
API gateway instance has 5m CPU > 80% for 5m.

#### ApiGatewayHighCPUWarning  
API gateway instance has 5m CPU > 65% for 5m. May require compute upgrade.

#### ApiGatewayNetworkWarning  
API gateway instance has network traffic > 3MB/sec for 5m. May evaluate dedicated network.

#### ApiGatewayNoAlertsScrapedWarning  
Prometheus can't scrape api_gateway on instance, is it running? Can Prometheus reach it?

#### ApiGatewayRequestCountLowWarning  
API gateway instance is not receiving traffic which can indicate the TIC can no longer reach this instance.

### Application Alerts
#### ApplicationErrorRateCritical  
Fired when nginx serves less than 95% of an application's traffic without an
error for 5m.

Visit the [Site Dashboard](http://grafana.vetsgov-internal/dashboard/db/site) to
narrow the cause to an application. Ensure that the application is serving
traffic appropriately, check the load balancer configuration is routing traffic
to all application servers. Escalate to the application development team if
necessary.

#### ApplicationLatencyCritical  
Fired when nginx serves less than 95% of an application's traffic in under two
seconds.

Visit the [Site Dashboard](http://grafana.vetsgov-internal/dashboard/db/site) to
evaluate latency characteristics. Check the resource consumption of application
instances. Verify that the latency increase does not affect multiple
applications, and escalate to the application development team.

#### ApplicationRequestCountCritical 
TODO: Expand on-call playbook for this error  
Application served 0 requests over 5m period, check for application infrastructure failure

#### ApplicationMetricsWarning  
The number of samples scraped by Prometheus from the StatsD exporter has exceeded the warning threshold. This check was put into place to catch a large number of unique metric-tag combinations being sent by the application suddenly.

You can view a graph over time as well as see a list of metrics by querying Prometheus directly with `scrape_samples_scraped{job="vets-api"}`.

If the change is gradual over a period of weeks, then we can simply increase the threshold. If the change is a marked change recently, it should be investigated. A change in the application likely is sending an unexpected number of unique metric-tag combinations.

#### ComponentLatencyWarning

The revproxy breaks down requests to the API into "components" which _most_ of the time map to the first part of the path after the `/v0/`. For example: requests to `/v0/user` get mapped to the `user` component.

Latency is measured in "buckets" that include requests returned in that number of seconds or lower. The buckets are currently: 2, 5, 10, 20, 40; they are defined in the revproxy configuration. This latency includes everything that our infrastructure and application may do with the request, including processing time if we make a request to a backend on the user's behalf.

When this alert fires, it's telling us that more than 95% of the requests are hitting in buckets higher than the bucket specified in the rule for that specific component. 

These warnings signify that user experience may be degraded below our expectations, and we should investigate the source of slowness. In some cases, we are simply passing latency from backends along to users. For a one-time firing of this warning, we should inquire with the backend support staff to find out if the service is degraded. If this warning consistently pops up for a given component and the source is in a related backend system, a decision should be made whether the threshold should simply be increased or the task moved to an asynchronous one running on the job queue to ensure we give users feedback in a timely fashion.

For details, you can select the component one the [Application Request Breakdown Dashboard](http://grafana.vfs.va.gov/dashboard/db/application-request-breakdown?orgId=1&var-data_source=Prometheus%20(Production)&var-application=api&var-component=address). In the case where we are passing along an upstream dependency's latency, there will also likely be a spike in latency for that backend on the [Backend Service Report Dashboard](http://grafana.vfs.va.gov/dashboard/db/backend-service-report?orgId=1&from=now-3h&to=now). There is not a mapping of components to backends known.

This rule is generic and covers any new or "unknown" components. If this is triggering for an `undefined` component, there may be a new component that's not covered in [the defined list of components](https://github.com/department-of-veterans-affairs/devops/blob/master/ansible/deployment/config/revproxy-vagov/vars/nginx_components.yml) and should be synced with vets-api.

#### ComponentUnprocessableEntityWarning  
Fires when more than 50% of requests are returning an HTTP 422. This indicates that a form on the website is likely unusable. This can come from a mismatch of expectations between front-end and back-end components to process a form, or a bug in form validation. Check Sentry for form validation errors for more details.

Historical context for this alert can be found in [this postmortem](https://github.com/department-of-veterans-affairs/vets.gov-team/blob/master/Postmortems/2019-04-12%20-%20Burial%20Pre-Need%20Submission%20Failures.md).

#### FacilityLocatorUpdateErrorsWarning  
The background job that syncs facilities from ArcGIS is failing and should be inspected. Check Sentry for exceptions from the worker classes. Also see https://github.com/department-of-veterans-affairs/devops/blob/master/docs/External%20Service%20Integrations/ArcGIS.com.md for ArcGIS information.

#### Form526FailureRateWarning

This warning was setup to catch errors in the submission process at the backend workers. If this warning is triggered, the cause of these failures should be triaged and investigated.

- You can see a count of overall submissions via a rake task [here](http://jenkins.vfs.va.gov/job/rake_tasks/job/vets-api-form526-submissions/)
- You can see a break down of the counts of errors via a rake task [here](http://jenkins.vfs.va.gov/job/rake_tasks/job/vets-api-form526-errors/)

This endpoint has historically seen fairly low amounts of traffic, so the percentage based alerts can be tricky. It alerts when the failure rate for the past 5 minute chunk of time has been greater than 30% for the past 30 minutes. If that's the case, there may be some systemic issue happening and categories of errors need to be investigated.

Further information about 526 overall can be found in the [product portfolio](https://github.com/department-of-veterans-affairs/vets.gov-team/tree/master/Products/Disability/Disability%20526EZ).

#### MaintenanceWindowErrorsWarning  
The background job that polls PagerDuty API for maintenance windows is failing consistently and should be inspected. Check Sentry for exceptions from the PollMaintenanceWindows worker.

### Auth Alerts
#### AuthErrorRateCritical  
Vets.gov authentication is provided via an integration with ID.me. An [overview of SAML authentication flow](https://github.com/department-of-veterans-affairs/vets.gov-team/blob/master/Products/Identity/Login/reference_documents/Auth/authentication_and_authorization.md#authn-flow)
provides some insight into where failures in the authentication process may occur.

Failures are tracked as either `auth_too_early`, `auth_too_late`, `clicked_deny`, or `unknown`.
This alert is triggered when the error rate is higher than expected for any one of these
variations. See the error description annotations for more information about the specific
issue that triggered the alert.

Check the [Site Authentication Dashboard](http://grafana.vetsgov-internal/dashboard/db/site-authentication) to
determine the error rate and error breakdown. `auth_too_late` and `clicked_deny` errors may
be caused in the course of normal user interaction. If these errors trigger an alert the threshold
may need reevaluated. `auth_too_early` errors may be representative of a [clock drift issue (NTP)](https://github.com/department-of-veterans-affairs/vets.gov-team/issues/561).

### Blackbox Alerts
TODO: Flesh out API gateway on-call playbook
#### BlackboxAPIGatewayReachabilityCritical  
API gateway endpoint reachability is < 95% for the past 5 minutes

#### BlackboxAPIGatewayReachabilityWarning  
API gateway endpoint reachability is < 95% for the past 5 minutes

#### BlackboxDNSReachabilityCritical  
The Blackbox Monitor makes DNS requests to check for expected responses from
a consumer DNS system (OpenDNS), and from the .gov and va.gov nameservers.
This alert is triggered when the consumer DNS is either not responding or does
not return the expected response for www.vets.gov, vets.gov,  or the
corresponding development or staging environment domain names.

Check the [Site
Dashboard](http://grafana.vetsgov-internal/dashboard/db/site?var-data_source=Prometheus%20(Production))
to determine if the .gov or va.gov nameservers are not responding appropriately,
and causing issues with the consumer DNS records. It's possible the consumer DNS
service is down and VA/.gov nameservers are still properly respondign to
requests. Check https://system.opendns.com/ (http://208.69.38.170/) for the
consumer DNS server status.

This alert is historically triggered when multiple VA nameservers fail to
respond to requests.

#### BlackboxDNSSECValidationCritical  
Google DNS is unable to validate DNSSEC for www.vets.gov

#### BlackboxMonitorCritical  
The BlackBox Monitor responsible for providing reachability metrics cannot be
scraped by the Prometheus server. While the site may be operational,
reachability cannot be confirmed.

Log in to VA's commercial AWS and check for availability. Confirm no changes
have been made to the IP address of the Blackbox Monitor instance, and that
Prometheus is able to reach the system. You should visit the Prometheus scrape
targets page to determine the error that Prometheus is encountering when trying
to scrape metrics from the system.

#### BlackboxSitePasswordProtection  
TODO: Expand on-call playbook for this error  
Password protection for target endpoint returning an invalid response

#### BlackboxSiteReachabilityCritical  
The Blackbox Monitor makes requests to each externally available component of
the VA.gov platform every ~30s. When less than 95% of these requests succeed
for over 5 minutes, this alert is triggered.

Check the [Site
Dashboard](http://grafana.vetsgov-internal/dashboard/db/site?var-data_source=Prometheus%20(Production))
to determine which component is failing. The `/health-check` endpoint tests reachability between
the Blackbox Monitor and reverse proxy without invoking any application or external service, so a
lack of reachability to this endpoint suggests an issue with VA network infrastructure or the reverse
proxies themselves. The [VAEC Connectivity dashboard] shows a view into the VA infrastructure.

This alert may be triggered when a lack of reachability information is available due to Blackbox Monitor
failure. This alert will be accompanied by `BlackboxMonitorCritical`.

#### BlackboxSiteReachabilityWarning  
This alert is similar to the above, but it raises a non-critical alert when a single host and path component has less than 95% success rate averaged over the past 5 minutes. The message text contains the host and path name which is causing the alert to fire, giving insight into which component should be investigated.

It is designed to catch longer-term issues where a site may have some sort of intermittent errors, but not fully down.

It may be coupled with a BlackboxSiteReachabilityCritical alert since they have similar thresholds, but should not generate a duplicate page (only a message in Slack).

#### BlackboxSiteReachabilityThroughTICWarning  
The Blackbox Monitor makes a request through each external IP address to ensure both paths are available. When one of these fails for 5 minutes, this alert is triggered. This check is similar to the above `BlackboxSiteReachabilityCritical` except that it uses each IP address forcing a request through each TIC and bypasses what is normally returned by DNS.

In the [TechnicalArchitectureOverview diagram](https://github.com/department-of-veterans-affairs/vets.gov-team/blob/master/Practice%20Areas/Engineering/TechnicalArchitectureOverview.md#reality), this represents a failure at either TIC East, TIC West. The alert will contain which CSR is associated with the TIC affected, you can also view it visually on the [Site Dashboard](http://grafana.vetsgov-internal/dashboard/db/csr). One CSR showing as up and another as down likely indicates an issue either on the VA network or with the CSR itself.

This alert may be accompanied by a `BlackboxSiteReachabilityCritical` alert. If so, that generally means that the DNS response continues to use the IP associated with the down network path, and is causing intermittent outages for users.

### Cloudwatch Exporter Alerts
#### CloudwatchExporterErrorCritical  
This alert is fired when the Cloudwatch Exporter encounters an issue scraping
metrics from the AWS Cloudwatch APIs for over 5m.

Check the `dsva-vetsgov-cloudwatch-exporter` IAM user's policy to ensure it has
access to CloudWatch. Ensure that the access keys provided to the Cloudwatch
Exporter are accurate. Check the AWS status dashboard for availability issues,
and escalate to AWS support if necessary.

#### CloudwatchExporterReachabilityCritical  
This alert is fired when Prometheus cannot scrape metrics from the
CloudwatchExporter for over 5 minutes.

Ensure that the Utility VPC's prometheus instance is able to connect to the
Cloudwatch Exporter's ELB, and that Security Groups are configured properly.
Check the Cloudwatch ELB and associated instances to ensure that there are
instances available.

### Cluster Alerts
#### DeploymentHostCountCritical  
Vets.gov related applications are served by a number of instances behind an AWS
elastic load balancer. They are managed by AWS autoscaling groups (ASG), which will
ensure that the number of instances within the load balancer is within a target
number.

If the ASG is unable to launch instances it believes are healthy (may be caused
by a failure to launch the application), and results in no instances capaable of
running the application, then this alert will be fired in response.

Check the ASG activity to see why instances are not being registered. Check the
application logs for evidence of failed startup. Check that the OS is capable of
booting and loaded properly.

#### DeploymentInstanceCyclingWarning

This warning indicates that Prometheus is seeing changes in instance ID's for a
particular "deployment". This typically is an indication that an auto-scale group is stuck
in a cycle where it is starting new instances which never become healthy so it kicks those out
and tries another instance which also never becomes healthy.

Search the [AWS console ASG page](https://console.amazonaws-us-gov.com/ec2/autoscaling/home?region=us-gov-west-1#AutoScalingGroups:view=details) for the deployment_name value returned and check on any ASGs to see
if you can see that happening in the Activity History panel.

If this is the case, check the load balancer associated with the ASG to see if it has a
reason for failing health checks, login to an instance to observe behavior, and/or check
CloudWatch Logs for the deployment to see if there are errors starting the service in
`/var/log/cloud-init-output.log` for the environment these instances are deployed into.

### CMS Alerts
TODO: Flesh out CMS on-call playbook
#### SiteReachableCritical  
The monitor probe to check site failed from the vets.gov utility network. There may be an issue loading content from Drupal for website builds.

### EC2 Alerts
#### ASGLowCPUCreditWarning  
At least one instance in auto-scaling group has less than 30 CPU Credits (the minimum at launch). Consider changing the instance type for this service. See http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/t2-credits-baseline-concepts.html

#### EC2PredictedLowCPUCreditWarning  
Instance will run out of CPU credits in less than 4 hours. Consider adding instances or changing instance type. See http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/t2-credits-baseline-concepts.html

### External Service Alerts
#### ExternalServiceAvailabilityCritical  
#### ExternalServiceAvailabilityWarning  
One of the external services integrations that our application depends on is not
available.

See the [External Services Status](http://grafana.vfs.va.gov/dashboard/db/external-service-status?orgId=1) dashboard for help in troubleshooting.

#### ExternalServiceMonitorCritical  
The system responsible for gathering metrics on external service integrations is
unavailable, and the monitoring system does not know if external services are
operating as expected.

Visit the [Production Prometheus](http://prometheus-prod.vetsgov-internal:9090/prometheus/targets) interface
and check the scrape targets. Ensure that the script-exporter process is running
on the prometheus servers.

### Jenkins Alerts
#### JenkinsMasterAvailabilityWarning  
Node exporter on Jenkins master could not be scraped, check status of Jenkins master instance and node exporter service.

#### JenkinsMasterHealthCheckWarning  
Jenkins HealthCheck score is 0. See https://wiki.jenkins.io/display/JENKINS/Metrics+Plugin for help in diagnosing.

#### JenkinsMasterMetricsUnscrapablekWarning  
The Jenkins metrics plugin is unreachable. This could indicate that Jenkins web interface is down or an issue with metrics collection.

#### JenkinsSwarmNodeAvailabilityWarning  
Jenkins Swarm instance node exporter could not be scraped, check status of Jenkins Swarm node and node exporter service.

### Kong Alerts
TODO: Flesh out API gateway on-call playbook
#### Upstream5xxErrorRateCritical  
Upstream service {{ $labels.service }} served {{ $value }}% 5xx errors, should be no greater than 5%

### Node Alerts
#### InstanceDiskUtilizationWarning  
Fired when the percentage of time per second operating on disk input/output
operations is higher than 50% for more than 10 minutes. This indicates higher
than expected disk activity for the application.

Evaluate disk utilization. The `iotop` utility (available via a yum package of
the same name) can be used to determine which processes are consuming disk.
Attempt to address at the process level, or, if the application is developed
internally escalate to the application development team for further evaluation.  

#### InstanceHighDiskIOClassDiskUtilizationWarning  
Instance under high disk utilization. We expect this instance to have 'higher than normal' disk IO, but this is exceptionally high.  See [InstanceDiskUtilizationWarning](#instancediskutilizationwarning)

#### InstanceEntropyLowCritical  
TODO: Expand on-call playbook for this error  
Instance entropy running low  

#### InstancePredictedEntropyDepletionCritical  
TODO: Expand on-call playbook for this error  
Predicted that instance will exhaust entropy in 30 minutes

#### InstanceHighCPUClassHighCPUWarning  
Fired when total CPU utilization of an instance exceeds 80% of capacity for more
than 3 hours. Applicable for instances with a `cpu_utilization` tag with "high"
value.

Check to see if other instances associated with this job are also experiencing
high CPU utilization. These instances are expected to operate under higher CPU
utilization than others. If this alert is common, but the instances are not under
higher than expected load, the alert may be tuned. If the instance is not operating
as expected due to the load, then an instance type upgrade may be required.

#### InstanceHighCPUWarning  
Fired when total CPU utilization of an instance exceeds 80% of capacity for more
than 5 minutes. Applicable for instances without a `cpu_utilization` tag.

Check to see if other instances associated with this job are also experiencing
high CPU utilization. If this is due to an increase in traffic, or is expected,
it may be necessary to increase the target size of the associated ASG where
applicable. If this instance is not part of an ASG, an upgrade to a larger
system may be required.

#### InstanceLowDiskRemainingCritical  
#### InstanceLowDiskRemainingWarning  
Fired when the current available free filespace on a system is less than 10% (warning) or 2% (critical)

Check the system disk to establish disk consumption characteristics. There may
be log files that are not being rotated, or the application may be writing data
to disk. Introduce measures to permanently reduce filesystem usage, or upgrade
the disk if necessary.

This warning may be accompanied by `InstancePredictedLowDiskRemainingWarning`
when disk consumption rate indicates the disk will fill within 8 hours, or
`InstancePredictedLowDiskRemainingCritical` if the current consumption rate is
even higher and the disk may fill within a 4 hour window.

#### InstancePredictedFileDescriptorUtilizationCritical  
#### InstancePredictedFileDescriptorUtilizationWarning  
Fired when the last hour of file descriptor utilization indicates that the total
available file descriptors will be exhausted within the next 8 hours (warning) 
or 4 hours (critical).

Use the `lsof` utility to check for open files and determine the cause. If the
server is accepting too many network connections increase the number of servers
available to the ASG when applicable to attempt to distribute traffic and bring
concurrency down.

Evaluate the `ulimit` to ensure that it's high enough for the servers' purpose.
Manual adjustments when necessary to prevent outage, but ensure these are
accompanied by configuration changes to prevent issues in the future.

#### InstancePredictedInodeExhaustionCritical  
#### InstancePredictedInodeExhaustionWarning  
TODO: Expand on-call playbook for this error  
Fired when predicted that the system will exhaust inodes within 8 hours (warning) 
or 4 hours (critical)

#### InstancePredictedLowDiskRemainingCritical  
#### InstancePredictedLowDiskRemainingWarning  
Fired when the last hour of disk consumption indicates that the free filespace
on a system will be ehxhausted within 8 hours (warning) or 4 hours (critical).

Check the system disk to establish disk consumption characteristics. There may
be log files that are not being rotated, or the application may be writing to
disk. Rotate/truncate logs where necessary to restore service if it's affected,
and ensure this is accompanied by a configuration update to prevent further
issues. If the application is consuming disk escalate to the application
development team for further evaluation.

#### InstancePredictedLowMemoryWarning  
Fired when the last hour of memory consumption indicates that the memory
available to the system will be exhausted within two hours.

Check the affected instance to ensure that this was not the result of
a temporary spike in memory that resolves on its own. If an application is
growing in memory usage over time at a predictable rate, escalate to the
application development team for further evaluation.

### Prometheus Alerts
#### PrometheusLocalStorageMemorySeriesWarning  
The number of time series that prometheus is managing is higher than expected.

Prometheus is tuned to run with fewer unique time series. Check that the number
of time series is expected. Attempt to find the source of the
time series increase and ensure that label values are not based on dynamic sources.

If Prometheus requires more time series, a hardware upgrade may be required to
increase memory availability. This should be accompanied with a change in the 
`storage.local.memory-chunks` configuration. See the [Prometheus Storage](https://prometheus.io/docs/operating/storage/)
documentation for more details.

See the https://github.com/department-of-veterans-affairs/vets.gov-team/blob/master/Postmortems/2017-02-21-prometheus-metrics-count.md
postmortem for some background on the addition of this error.

#### PrometheusSamplesScrapedWarning  
The number of samples scraped by the prometheus instance exceeds expectations.

Check the number of scraped samples with the `scrape_samples_scraped` metric per job in 
the [prometheus dashboard](http://prometheus-prod.vetsgov-internal:9090/prometheus/graph?g0.range_input=1d&g0.expr=scrape_samples_scraped&g0.tab=0) (link to prod, adjust for other envs if necessary)
to see a comparison with other jobs, most likely you'll see 
some metric climbing substantially higher than others. If this growth is desired, check the
prometheus scrape duration to ensure that Prometheus is able to handle the new normal and adjust this threshold. 
The cause for this situation is most likely a growth in unique labels being generated in response to user input,
Prometheus does not handle that well. As above, see [this postmortem](https://github.com/department-of-veterans-affairs/vets.gov-team/blob/master/Postmortems/2017-02-21-prometheus-metrics-count.md) for additional context.

#### ApplicationEnvironmentPrometheusWarning  
Prometheus EC2 autodiscovery from the instance on the Utility VPC populates
a list of prometheus node scrape endpoints from application environments. This
alarm is fired when the endpoints for prometheus instances in staging and
development environments cannot be scraped for 2m.

If accompanied by a `ProductionPrometheusCritical` alert, it's possible that the
utility VPC can no longer communicate with the application environment VPCs, or
that a security group change has prevented traffic from the utility VPC to
application environment prometheus instances.

If not accompanied by another alarm, check that the environment's prometheus
instance is running and responding to requests for the metrics endpoint on port
9090. Ensure that security group configuration permits traffic from the utility
VPC. Check that the utility VPC prometheus instance is able to reach the metrics
endpoint on the production VPC.

#### ProductionPrometheusCritical  
Prometheus EC2 autodiscovery from the instance on the Utility VPC populates
a list of prometheus node scrape endpoints from application environments. This
alarm is fired when the endpoints for prometheus instances in the production
environment cannot be scraped for 2m.

Check that the production prometheus instance is up and responding to requests
on the metrics endpoint on port 9090. Ensure that the security group
configuration permits traffic from the utility VPC. Check that the utility VPC
prometheus instance is able to reach the metrics endpoint on the production VPC.

### RDS Alerts
TODO: Flesh out RDS on-call playbook
#### RDSPredictedLowStorageRemainingWarning  
Instance ${{$labels.dbinstance_identifier}} will exhaust storage space in 24 hours

#### RDSLowStorageRemainingWarning  
Instance {{$labels.dbinstance_identifier}} has low remaining disk

#### RDSCPUUtilizationAverageWarning  
RDS node {{$labels.dbinstance_identifier}} > 60% CPU utilization

### Revproxy Alerts
#### RevproxyHighCPUCritical  
This alert is fired when a reverse proxy instance exceeds 80% CPU utilization
for a period of 5m. The reverse proxy is responsible for SSL termination, which
may lead to higher CPU utilization when request counts increase.

This alert was added in response to reachability issues caused by high CPU
utilization due to an increase in sampled metrics. The prometheus lua nginx
integration uses a shared dictionary to store metric values, and if the number
of metrics increases considerably it consumes high CPU and also blocks requests.

Actions have been taken to mitigate this issue, but it's worth checking
http://prometheus-prod.vetsgov-internal:9090/prometheus/graph?g0.range_input=1h&g0.expr=scrape_samples_scraped%7Bjob%3D%22revproxy%22%7D&g0.tab=0 to see if the number
of metrics has increased considerably. If so, restart the openresty processes on the
revproxies and then evaluate what's causing the increase in scraped metric count.

You'll likely get the `RevproxyHighCPUWarning` alert prior to this one. Follow
tips there as well for further diagnosis.

#### RevproxyHighCPUWarning  
This alert is fired when a reverse proxy instance exceeds 65% CPU utilization
for a period of 5m. The reverse proxy is responsible for SSL termination, which
may lead to higher CPU utilization when request counts increase.

Check the reverse proxy dashboard to see if all reverse proxies within the
environment are operating under the same increase in CPU. Check for
a corresponding increase in traffic. An upgrade to an instance type with more
compute resources may be required. If not all instances are affected, check that
traffic is routed evenly to all systems, and escalate to NSOC for CSR
configuration updats if necessary. Restart the openresty service to attempt to
restore service, or rebuild the instance if it's possible configuration changes
were made out of sync with provisioning source.

#### RevproxyNetworkWarning  
Fired when a reverse proxy instance sustains more than 3MB/s traffic for
a period of 5 minutes.

This level of traffic may warrant an upgrade to a system with more
dedicated network capacity. Check the CSR traffic to ensure that it is within
licensing bounds. If accompanied by `CSRTotalTrafficWarning` then traffic may
warrant a CSR license upgrade in addition to a system upgrade.

#### RevproxyNoAlertsScrapedWarning  
Prometheus can't scrape nginx on instance, is it running? Can Prometheus reach it?

#### RevproxyRequestCountLowWarning  
Instance is not receiving any vets.gov related traffic which can indicate the TIC can no longer reach this instance.

### Site Alerts
#### SiteErrorRateCritical  
Fired when nginx serves less than 95% of site traffic without an error for 5m.

Visit the [Site Dashboard](http://grafana.vetsgov-internal/dashboard/db/site) to
determine if this is the result of a specific application. Ensure that the
application is serving traffic appropriately, or escalate to the application
development team. If all applications are affected, check the nginx
configuration on the revproxy systems to ensure that they are serving traffic
correctly.

#### SiteLatencyCritical  
TODO: Expand on-call playbook for this error  
Less than 95% of site requests are being served in under 2 seconds, should be over 95%.

#### SiteRequestCountCritical  
Fired when nginx (across all of an environment's revproxy instances) is serving
no requests for 5m.

Corroborate with reachability checks. Check that the reverse proxies are online
and accepting traffic. Check that nginx (via openresty) is configured and
running on each reverse proxy. Check that the revproxy is accepting traffic from
the CSRs, and that the security groups permit this traffic.

### Utility Services Alerts  
TODO: Flesh out utility services on-call playbook
#### NamedServiceCritical  
Named, internal DNS service, is unavailble.

#### UtilityServiceAvailabilityCritical  
Grafana/Sentry web interface is unavailable.

## Blackbox Monitor

An instance in `us-east-1e` runs the Prometheus Blackbox Exporter, which tests
application and DNS reachability. An exporter for each environment is configured
there, and application environment Prometheus instances scrape their metrics
endpoints to collect reachability information.

Configuration files defining blackbox tests are in the
[blackbox-configure templates](https://github.com/department-of-veterans-affairs/devops/tree/master/ansible/deployment/config/blackbox-exporter)
directory.

## Cloudwatch Exporter

The Cloudwatch Exporter provides Prometheus with Cloudwatch metrics it retreives
by polling the Cloudwatch APIs. These metrics may be used to monitor instances
that cannot export node metrics such as CSRs or RDSs, or for monitoring AWS
specific metrics. It runs with an ASG and ELB providing service over multiple
AZs. Since Cloudwatch metrics are provided across an entire AWS account, only
one cluster of these systems is required, and it runs within the Utility VPC.

## Cloudwatch Alarms

Cloudwatch Alarms supplement the Prometheus alertmanager system, and fire alerts
for instances where it is difficult to manage alerts with Prometheus. This is
useful for alerts around AWS-specific features that are not easily monitored
with Prometheus. For most instances the preferred route is to configure an alert
with Prometheus.

### NodeExporterErrorCount

Cloudwatch-logs based alarm indicating that an error was found in the node
exporter log stream for the environment. An alert from PagerDuty indicates that
an alert was found in either the utility or production environment.

This alert indicates a failure to scan a node for required metrics. This may be
most prevelant when the scrape requires communication with a third-party server,
such as the case for NTP metrics.

Log in to the AWS Console, and check the `dsva-vetsgov-{{ env
}}/var/log/prometheus/node_exporter.log` group with this filter expression:
`[time, level="level=error", msg]`.

### PrometheusErrorCount

Cloudwatch-logs based alarm indicating that an error was found in the prometheus
log stream for the environment. An alert from PagerDuty indicates that an alert
was found in either the utility or production environment.

This alert may indicate that an error occurred in one of the Prometheus components,
including the Prometheus server, the AlertManager, or the Script Exporter. Any of
these cases may indicate that the system is not being monitored properly, and thus
warrants a page event.

Log in to the AWS Console, and check the utility-prometheus and prod-prometheus log
groups with this filter expression: `[time, level="level=error", msg]`.

### Root Account Usage

The AWS root account was used to perform an action. Escalate immediately to
Shawn Arnwine for investigation.
